#  Local Chatbot using Ollama + Flask + Docker

This project demonstrates how to build a **local AI chatbot** using:

- **Ollama** (LLaMA 3 model)
- **Python Flask backend**
- **Docker** container
- **Simple HTML UI**

---

##  Features

- Runs fully offline
- Uses Ollama for local LLaMA inference
- Dockerized for easy deployment
- Simple web UI

---

##  Installation

### 1. Install Ollama
Download: https://ollama.com  
Then pull a model:

```bash
ollama pull llama3


*****
Key Features

1)Fast, local inference

2)100% private — no cloud required

3)Fully Dockerized

4)Browser-based chat UI

5)Easy to extend (e.g., RAG, vector DB, memory, auth)



Folder Structure
├── chatbot.py
├── requirements.txt
├── Dockerfile
├── templates/
│   └── index.html
└── README.md


******Perfect For

Learning how to integrate LLMs with Python

Local AI/ML experimentation

Building your own ChatGPT-like app

Deploying AI tools internally at companies

Showcasing skills in AI + Docker + backend development*****

Steps
-->Open terminal and type 
git clone https://github.com/yourusername/ollama-flask-chatbot.git (They can also clone it via Git (developers do this)
-->If run it locally
docker build -t flask-ollama-chatbot .
docker run -p 5000:5000 \
  -v ~/.ollama:/root/.ollama \
  -v /var/run/ollama:/var/run/ollama \
  flask-ollama-chatbot
-->Then Open
http://localhost:5000







